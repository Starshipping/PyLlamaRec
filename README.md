
# PyLlamaRec: Two-Stage Recommendation using Large Language Models for Ranking\n\nThis repository is the PyTorch implementation for the PGAI@CIKM 2023 paper **LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking [[Paper](https://arxiv.org/abs/2311.02089)]**.\n\n![Method](media/method.png)\n\nWe propose a two-stage framework, LlamaRec, which uses large language models for ranking-based recommendation. Sequential recommenders retrieve candidates based on the user interaction history. Then, both history and retrieved items are fed to the LLM via a carefully designed prompt template. We adopt a verbalizer-based approach that transforms output logits into probability distributions over the candidate items. Thus, LlamaRec can efficiently rank items without generating long text, achieving superior performance in both recommendation performance and efficiency.\n\n## Requirements\n\nPytorch, transformers, peft, bitsandbytes and more. See requirements.txt for our detailed running environment.\n\n## How to run PyLlamaRec\n\nThe command below starts the training of the retriever model LRURec\n```bash\npython train_retriever.py\n```\nYou can set additional arguments like weight_decay to change the hyperparameters. You will be prompted to select a dataset from ML-100k, Beauty and Games. Evaluation is automatically performed on completion.\n\nTo train the ranker model based on Llama 2, run the following command:\n```bash\npython train_ranker.py --llm_retrieved_path PATH_TO_RETRIEVER\n```\nPlease specify PATH_TO_RETRIEVER with the retriever path from the previous step. You will need access to meta-llama/Llama-2-7b-hf on the HF hub. Evaluation is performed once training is completed. All weights and results are saved under ./experiments.\n\n## Performance\n\nThe table below reports our results.\n\n![Performance](media/performance.png)\n\n## Citation\n\nRefer to the following citations for the published papers:\n```
@article{yue2023linear,\n  title={Linear Recurrent Units for Sequential Recommendation},\n  author={Yue, Zhenrui and Wang, Yueqi and He, Zhankui and Zeng, Huimin and McAuley, Julian and Wang, Dong},\n  journal={arXiv preprint arXiv:2310.02367},\n  year={2023}\n}\n@article{yue2023llamarec,\n  title={LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking},\n  author={Yue, Zhenrui and Rabhi, Sara and Moreira, Gabriel de Souza Pereira and Wang, Dong and Oldridge, Even},\n  journal={arXiv preprint arXiv:2311.02089},\n  year={2023}\n}
```